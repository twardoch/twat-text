[build-system]
requires = [
    "hatchling>=1.21.0",
    "hatch-vcs>=0.3.0",
]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/twat_text"]

[project]
name = "twat-text"
dynamic = ["version"]
description = ""
readme = "README.md"
requires-python = ">=3.10"
license = "MIT"
keywords = []
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]

dependencies = [
    "twat>=1.0.0",
]


[[project.authors]]
name = "Adam Twardoch"
email = "adam+github@twardoch.com"


[project.urls]
Documentation = "https://github.com/twardoch/twat-text#readme"
Issues = "https://github.com/twardoch/twat-text/issues"
Source = "https://github.com/twardoch/twat-text"




[tool.hatch.version]
source = "vcs"

[tool.hatch.version.raw-options]
version_scheme = "post-release"

[tool.hatch.build.hooks.vcs]
version-file = "src/twat_text/__version__.py"



[tool.hatch.envs.default]
dependencies = [
    "pytest",
    "pytest-cov",
    "mypy>=1.0.0",
    "ruff>=0.1.0",
]


[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_text --cov=tests {args:tests}"
type-check = "mypy src/twat_text tests"
lint = ["ruff check src/twat_text tests", "ruff format src/twat_text tests"]


[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]


[tool.hatch.envs.lint]
detached = true
dependencies = [
    "mypy>=1.0.0",
    "ruff>=0.1.0",
]


[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/twat_text tests}"
style = ["ruff check {args:.}", "ruff format {args:.}"]
fmt = ["ruff format {args:.}", "ruff check --fix {args:.}"]
all = ["style", "typing"]


[tool.ruff]
target-version = "py310"
line-length = 88
lint.extend-select = [
    "A",
    "ARG",
    "B",
    "C",
    "DTZ",
    "E",
    "EM",
    "F",
    "FBT",
    "I",
    "ICN",
    "ISC",
    "N",
    "PLC",
    "PLE",
    "PLR",
    "PLW",
    "Q",
    "RUF",
    "S",
    "T",
    "TID",
    "UP",
    "W",
    "YTT",
]
lint.ignore = [
    "ARG001", # Unused function argument
    "E501",   # Line too long
    "I001",
]


[tool.ruff.per-file-ignores]
"tests/*" = ["S101"]


[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true


[tool.coverage.run]
source_pkgs = ["twat_text", "tests"]
branch = true
parallel = true
omit = [
    "src/twat_text/__about__.py",
]


[tool.coverage.paths]
twat_text = ["src/twat_text", "*/twat-text/src/twat_text"]
tests = ["tests", "*/twat-text/tests"]


[tool.coverage.report]
exclude_lines = [
    "no cov",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]

[project.optional-dependencies]
test = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-xdist>=3.5.0",  # For parallel test execution
    "pytest-benchmark[histogram]>=4.0.0",  # For performance testing
]

dev = [
    "pre-commit>=3.6.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
]

all = [
    "twat>=1.0.0",
]

[tool.hatch.envs.test]
dependencies = [".[test]"]

[tool.hatch.envs.test.scripts]
test = "python -m pytest -n auto {args:tests}"
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_text --cov=tests {args:tests}"
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

[tool.pytest.ini_options]
markers = ["benchmark: marks tests as benchmarks (select with '-m benchmark')"]
addopts = "-v -p no:briefcase"
testpaths = ["tests"]
python_files = ["test_*.py"]
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
asyncio_mode = "auto"

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min",    # Minimum time
    "max",    # Maximum time
    "mean",   # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr",    # Inter-quartile range
    "ops",    # Operations per second
    "rounds", # Number of rounds
] 